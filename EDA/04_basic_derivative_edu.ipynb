{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "requested-workshop",
   "metadata": {
    "id": "monetary-problem"
   },
   "source": [
    "## *SECTION 1 / SPRINT 1 / NOTE 4*\n",
    "\n",
    "---\n",
    "\n",
    "# 기초 미분 (Basic Derivative)\n",
    "\n",
    "## 학습 목표 \n",
    "\n",
    "- 최적화와 미분의 관계에 대해 알아봅니다\n",
    "- 미분, 편미분, Chain Rule의 차이를 이해하고 계산을 할 수 있게 됩니다.\n",
    "- 도함수를 파이썬으로 직접 구현하거나 scipy 라이브러리를 활용해서 구할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solved-sense",
   "metadata": {
    "id": "continued-istanbul"
   },
   "source": [
    "# 미분 (Derivative)\n",
    "<img src = 'https://upload.wikimedia.org/wikipedia/commons/thumb/0/0f/Tangent_to_a_curve.svg/1200px-Tangent_to_a_curve.svg.png' width=\"400\" height=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "german-carpet",
   "metadata": {
    "id": "rapid-cross"
   },
   "source": [
    "데이터 사이언스를 공부한다면 수학은 피할 수 없는 숙명과도 같은 존재입니다. <br>\n",
    "특히 미분이란 단어가 생소하신 분들도 많이 계실 것 입니다. 혹은 미분의 ㅁ만 들어도 소름이 돋으시는 분들도 계실 수도 있습니다.<br>\n",
    "하지만 미분은 여러분이 생각하시는 것만큼 무시무시하고 어려운 개념이 아닙니다. <br>\n",
    "오늘은 미분이 어떤 것인지를 이해하고 연습을 통해서 미분이라는 벽을 넘어보도록 하겠습니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pending-vegetable",
   "metadata": {
    "id": "supported-employer"
   },
   "source": [
    "## 미분이란?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "leading-consistency",
   "metadata": {
    "id": "white-vocabulary"
   },
   "source": [
    "미분이란 단어는 작을 미(微)와 나눌 분(分). 즉 \"작게 나눈다\"라는 의미입니다. \n",
    "무엇을 작게 나누는 것일까요? 바로 **함수**입니다.\n",
    "\n",
    "### 함수?\n",
    "<img src = \"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3b/Function_machine2.svg/2880px-Function_machine2.svg.png\" width='300' height='400'>\n",
    "함수는 간단하게 x라는 값을 넣으면 f(x)라는 결과값을 내보내주는 기계를 생각하시면 됩니다.<br>\n",
    "자주 쓰이는 예시는 세탁기를 생각해보시면 됩니다. `세탁물(x)`을 넣고, 버튼을 누르면 `깨끗해진 세탁물(f(x))`을 내보내주는 것이죠.\n",
    "<br>\n",
    "\n",
    "#### 그럼 함수를 작게 나눈다는 것은 어떤 의미일까요?\n",
    "함수를 작게 나눈다는 것은 warm-up 영상에서 보셨던 것처럼 X의 값을 아주 아주 미세하게 변화 시킨 후에 입력했을 때 (예를 들면 0.00000000000000000001 혹은 더 0에 최대한 가깝게) 그 결과값이 어떻게 바뀌는지를 보는 것이 미분입니다.<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "아래의 그림처럼 $\\Delta x$를 점점 0 에 가깝게해서, 순간의 변화량을 측정하고자 하는것이 더 구체적인 목표라 할 수 있습니다.\n",
    "\n",
    "<img src='https://upload.wikimedia.org/wikipedia/commons/c/cc/Tangent_animation.gif' width='300' height='300'>\n",
    "\n",
    "그림을 보시면 우리가 계산하고자 하는 것은 $\\Delta x$가 한없이 0에 가까워질 때의 기울기를 계산하고자 하는 것이죠.\n",
    "\n",
    "원론적인, 수학적인 의미를 파고 들면 끝이 없지만 일단 **미분**을\n",
    "\n",
    "> 특정한 파라미터 값 (`input, x`)에 대해서 나오는 결과값(`output, y`)이 변화하는 정도를 (0에 가까운 부분을 찾기 위해) 계산하는 것.\n",
    "\n",
    "으로 이해하시면 됩니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invisible-latvia",
   "metadata": {
    "id": "compliant-developer"
   },
   "source": [
    "## 미분이랑 데이터 사이언스는 뭔 상관이 있는건가요?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standard-thickness",
   "metadata": {
    "id": "accurate-armstrong"
   },
   "source": [
    "미분은 그럼 데이터 사이언스를 배우는데 왜 필요한 것일까요?\n",
    "아래와 같은 데이터 분포가  있다고 가정해보겠습니다.\n",
    "<br>\n",
    "오늘은 Simple Linear Regression (단순선형회귀, Section 2에서 더 상세하게 다루게 됩니다) 모델을 기준으로 예시를 들어보겠습니다. <br>\n",
    "단순선형회귀모델이란 전문용어에 당황하실 필요 없습니다. 간단하게 표현하면 **x로 y를 예측할 수 있는 하나의 선을 그린다**라고 생각하시면 됩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "difficult-quebec",
   "metadata": {
    "id": "broad-swedish",
    "outputId": "ea28829e-041f-4b95-9def-75ae959a4b74"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Oldcar\\anaconda3\\envs\\py38r40\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 랜덤하게 평균0, 표준편차1의 가우시안 표준정규분포 난수 x, y를 50개씩 뽑습니다\n",
    "np.random.seed(42)  # 동일한 결과를 보기 위해 시드를 고정합니다. https://numpy.org/doc/stable/reference/random/generated/numpy.random.seed.html\n",
    "x = np.random.randn(50)\n",
    "y = np.random.randn(50)\n",
    "\n",
    "# 산점도를 통해 x,y를 시각화 합니다.\n",
    "sns.regplot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expected-frontier",
   "metadata": {
    "id": "strategic-spider"
   },
   "source": [
    "x를 넣었을 때, y 값을 예측하는 선형 모델은 다음과 같이 표현할 수 있습니다 : \n",
    "\n",
    "$\\hat y = a + b X$\n",
    "\n",
    "여기서 $\\alpha$는 y-절편 (y-intercept), $\\beta$는 기울기 (slope) 입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interior-mouth",
   "metadata": {
    "id": "juvenile-module"
   },
   "source": [
    "그럼 모델이 더 정확하게 실제값을 예측하기 위해서는 어떻게 해야할까요?<br>\n",
    "\n",
    "일단 주어진 데이터 X를 넣었을 떄 모델이 예측하는 예측값과 실제값 간의 차이(**Error, $\\varepsilon$**)를 계산한 다음,  여러 모델 중 **Error**(모델에서 예측하는 예측값과 실제값 (y)의 차이)가 가장 작은 모델을 선택하는 방법을 통해, **가장 좋은 모델**을 선택 할 수 있습니다.\n",
    "\n",
    "여기서 이 과정은 $f(a,b) = \\varepsilon$ 로 표현 될 수 있으며, 오차 함수인 $\\varepsilon$을 최소화 하는 $a,b$를 찾는 것이 머신러닝(`Linear regression`)의 목표입니다.\n",
    "\n",
    "선형회귀모델의 경우 오차 함수는 보통 [Mean Squared Error](https://developers.google.com/machine-learning/crash-course/descending-into-ml/training-and-loss)를 쓰는데요. \n",
    "\n",
    "오차 함수를 최소화하는 $a,b$를 구하기 위해서 우리는 미분을 사용합니다!\n",
    "\n",
    "미분을 통해서 오차 함수의 도함수($f'(x)$)가 0이 되는 부분 (즉 변화율이 0인 부분)을 찾아서 오차 함수가 최소화되는 $a,b$을 찾는 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "associate-roller",
   "metadata": {
    "id": "passive-moldova"
   },
   "source": [
    "## 미분 공식 w/ Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annual-telephone",
   "metadata": {
    "id": "frank-arthritis"
   },
   "source": [
    "세상에는 다양한 함수가 존재하기 때문에 미분을 하는 방법 또한 매우 많이 존재합니다.\n",
    "\n",
    "미분의 원리에 대해 깊게 파고들면서 어떻게 함수를 미분해야하는지 이해해가면서 도함수를 도출해내는 것은 이후에 조금 더 복잡한 함수를 다룰 때를 대비해서 매우 좋은 생각 입니다.\n",
    "\n",
    "하지만 아쉽게도 우리에게 주어진 시간이 너무나도 짧기 때문에 오늘은 대표적인 지름길 몇가지와 여러분이 앞으로 필요하게 될 중요한 방법 몇가지만 다루도록 하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unexpected-story",
   "metadata": {
    "id": "israeli-ecology"
   },
   "source": [
    "기본적으로 미분은 다음의 공식을 활용합니다: \n",
    "\n",
    "  $f'(x) = {f(x + \\Delta x) - f(x) \\over \\Delta x}$, $\\Delta x \\rightarrow 0$\n",
    "    \n",
    "하지만 실제로 0으로 나눌 수는 없기 때문에 0에 매우 근사한 값을 사용하게 됩니다. 보통 $1e-5$ 을 사용하며, 이러한 접근 방식을 `numerical method` 라는 방법으로 표현하기도 합니다.\n",
    "\n",
    "한편, `numerical method`에서는 조금 더 정확한 측정을 위해 분자에 $f(x + \\Delta x) - f(x - \\Delta x) \\over 2\\Delta x$ 를 사용하기도 합니다.\n",
    "\n",
    "그럼 머신러닝에서 대표적으로 쓰이는 미분 공식들에 대해 알아보겠습니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protected-glossary",
   "metadata": {
    "id": "manual-adult"
   },
   "source": [
    "1.  $f(x)$ = 상수 $\\rightarrow$ $f'(x)$ = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neither-objective",
   "metadata": {
    "id": "protective-there"
   },
   "source": [
    "f'(x)가 상수 (constant)인 경우에는 x를 아무리 늘리거나 줄여도 늘 같은 숫자이기 때문에 변화가 전혀 없습니다. 그 말은 즉 변화율이 0이기 때문에 미분계수도 늘 0입니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resistant-insert",
   "metadata": {
    "id": "associate-suggestion",
    "outputId": "c0c313f4-4571-4036-902d-fb45e5a1a77b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 79,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예시 1 : Numerical Method\n",
    "\n",
    "# f(x) = 5\n",
    "def f(x):\n",
    "    return 5\n",
    "\n",
    "def numerical_derivative(fx, x):\n",
    "    delta_x = 1e-5\n",
    "\n",
    "    return (fx(x + delta_x) - fx(x)) / delta_x\n",
    "\n",
    "print(numerical_derivative(f, 1))\n",
    "\n",
    "# 예시 2 : Scipy의 derivative 활용\n",
    "from scipy.misc import derivative\n",
    "\n",
    "# 두 방법의 결과값 비교\n",
    "derivative(f,1, dx=1e-6) == numerical_derivative(f, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "three-consultancy",
   "metadata": {
    "id": "bizarre-violence"
   },
   "source": [
    "2. $f(x) = ax^{n}$ $\\rightarrow$ $f'(x) = an{x}^{(n-1)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunrise-capitol",
   "metadata": {
    "id": "rubber-collapse"
   },
   "source": [
    "`Power Rule`로도 알려져있는 미분법입니다. x 기준으로 n승을 미분할 경우 n을 내려보내서 곱해준 후, 이후에 n승에서 \"하나\"를 가져왔기 때문에 빼주는 방법입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foreign-pharmaceutical",
   "metadata": {
    "id": "careful-natural"
   },
   "source": [
    "예시로 $f(x) = 3x^4 + 10$ 를 미분해보겠습니다.\n",
    "\n",
    "먼저 4승에서 하나를 내려보내서 앞에 있는 3과 곱해줍니다. (10은 상수이기 때문에 미분을 하면 0이 됩니다)<br>\n",
    "$f'(x) = (4*3)x^4$\n",
    "\n",
    "이후에는 4승에서 1을 빼줍니다 (\"빌려줬기 때문에\")<br>\n",
    "$f'(x) = (4*3)x^{4-1}$\n",
    "\n",
    "최종적으로 f(x)의 도함수는 이렇게 계산됩니다.<br>\n",
    "$f'(x) = 12x^3$\n",
    "\n",
    "x = 2일 때 f'(x)는 다음과 같습니다<br>\n",
    "$f'(2) =  96$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "contained-cycle",
   "metadata": {
    "id": "scenic-legislature",
    "outputId": "0c5fd12d-1a26-4e1b-c130-9fe5ee874b41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96.0007200028201\n",
      "96.0000000031158\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 같은 결과를 numerical method를 활용해서 계산해보겠습니다\n",
    "def f(x):\n",
    "    return 3*(x**4) + 10\n",
    "\n",
    "def numerical_derivative(fx, x):\n",
    "    delta_x = 1e-5\n",
    "\n",
    "    return (fx(x + delta_x) - fx(x)) / delta_x\n",
    "\n",
    "print(numerical_derivative(f, 2))\n",
    "\n",
    "# 예시 2 : Scipy의 derivative 활용\n",
    "from scipy.misc import derivative\n",
    "\n",
    "# 두 방법의 결과값 비교\n",
    "print(derivative(f,2, dx=1e-5))\n",
    "derivative(f,2, dx=1e-5) == numerical_derivative(f, 2)  #delta X의 값은 같지만 Rounding 에러로 인해 두 결과가 미묘하게 다른 것을 확인하실 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solid-testimony",
   "metadata": {
    "id": "psychological-queue"
   },
   "source": [
    "3. $f(x) = e^x$ $\\rightarrow$ $f'(x) = e^x$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conscious-essence",
   "metadata": {
    "id": "apart-production"
   },
   "source": [
    "지수 함수의 경우에는 도함수 역시 지수 함수입니다. 어떻게 도함수가 같은지를 증명하는 글을 Reference에 올려놨습니다. 시간이 되실 때 꼭 참고해보시길 바랍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patent-shannon",
   "metadata": {
    "id": "chief-buffer"
   },
   "source": [
    "4. $f(x) = lnx$ $\\rightarrow$ $f'(x) = {{1} \\over {x}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "present-fisher",
   "metadata": {
    "id": "distant-climb"
   },
   "source": [
    "자연 로그의 미분은 이후에 여러분이 Section 2에서 배우게 될 Logistic Regression이나 Section 4에서 배우게 될 신경망의 활성 함수인 sigmoid 함수를 미분할 때 상당히 편하게 미분을 할 수 있도록 도와줍니다.\n",
    "\n",
    "sigmoid 함수에 자연로그를 씌움으로서 미분을 훨씬 수월하게 할 수 있게 되기 때문입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exclusive-palestinian",
   "metadata": {
    "id": "owned-norman"
   },
   "source": [
    "## 편미분 (Partial Derivative) \n",
    "\n",
    "상당히 많은 머신러닝의 Error 함수는 여러개의 파라미터 값을 통해 결정됩니다.\n",
    "\n",
    "이때 쓰이는 것이 편미분인데요, 파라미터가 2개 이상인 Error 함수에서 **우선 1개의 파라미터에 대해서만 미분을 하자** 라는 목적으로 다른 변수들을 상수 취급 하는 방법을 말합니다.\n",
    "\n",
    "계산 방법을 간단한 예시를 통해 알아보겠습니다.\n",
    "\n",
    "$$ f(x,y) = x^2 + 2xy + y^2$$\n",
    "\n",
    "$${ {\\partial f(x,y)} \\over {\\partial x} } = {{\\partial {(x^2 + 2xy + y^2)} } \\over {\\partial x}} = 2x + 2y$$\n",
    "\n",
    "이렇게 y는 상수로 취급하고 x를 기준으로만 미분하거나 반대로 x를 상수 취급하고 y를 기준으로 미분하는 것이 편미분 입니다.\n",
    "\n",
    "여전히 편미분이 많은 분들에게 와닿지 않을 거라고 생각합니다.\n",
    "\n",
    "편미분이 실생활에서 쓰이는 예를 $f=ma$라는 유명한 공식을 통해 설명해보겠습니다.\n",
    "\n",
    "<img src='https://i.imgur.com/jBdd0VE.jpg' width = '500'>\n",
    "\n",
    "차량이 받는 힘(충격) = $f$(차체의 무게, 차의 가속도)\n",
    "\n",
    "현재 차의 무게는 그대로 두고 가속도에 변화를 주는 경우 충격은 어떻게 변화하는가? $\\rightarrow$ ${\\partial 충격 \\over \\partial 가속도 }$\n",
    "\n",
    "현재 가속도는 그대로 두고 차의 무게에 변화를 주는 경우 충격은 어떻게 변화하는가? $\\rightarrow$ ${\\partial 충격 \\over \\partial 질량 }$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incident-magic",
   "metadata": {
    "id": "general-assurance"
   },
   "source": [
    "### 예제\n",
    "$f(x,y) = x^2 + 4xy + 9y^2$라는 함수의 $f'(1, 2)$의 값을 계산해보겠습니다.\n",
    "\n",
    "이를 위해서 해야 하는 것은 다음과 같습니다 : \n",
    "\n",
    "1. $x$에 대해 편미분\u001f \n",
    "\n",
    "$\\partial f(x,y) \\over \\partial x$ = $2x + 4y$\n",
    "\n",
    "${f'(1, 2) \\over \\partial x}$ = $2 \\cdot (1) + 4 \\cdot (2) = 10$\n",
    "\n",
    "2. $y$에 대해 편미분\n",
    "\n",
    "$\\partial f(x,y) \\over \\partial y$ = $4x + 18y$\n",
    "\n",
    "${f'(1, 2) \\over \\partial y}$ = $4 \\cdot 1 + 18 \\cdot 2 = 40$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endless-shield",
   "metadata": {
    "id": "lonely-survival"
   },
   "source": [
    "# Chain Rule\n",
    "\n",
    "`Chain Rule`이란 **함수의 함수**를 미분하기 위해 사용하는 방식입니다. 이를 **합성함수** 라고 부르기도 하며\n",
    "\n",
    "공식은 다음과 같습니다. \n",
    "\n",
    "$F(x) = f(g(x))$\n",
    "\n",
    "$F'(x)$ $\\rightarrow$ $f'((g(x)) \\cdot g'(x)$\n",
    "\n",
    "흔히 Chain Rule을 양파까기에 비유를 많이 하는데요. \n",
    "양파 껍질을 깔 때 밖에서부터 껍질을 까듯이, Chain Rule 역시 미분을 할 때 바깥 함수($f(x)$)부터 미분을 한 후에 안에 있는 함수 ($g(x)$)를 미분합니다.\n",
    "\n",
    "\n",
    "### 예제\n",
    "\n",
    "$F(x) = (2x^3 + 7)^6 $ 를 x에 대해 미분하려는 경우\n",
    "\n",
    "$f(x) = x^6, g(x) = 2x^3 + 7$로 설정\n",
    "\n",
    "$F'(x) = 6(2x^3 + 7)^5 \\cdot 6x^2$\n",
    "\n",
    "`Chain Rule`은 `Deep learning`의 핵심 개념 중 하나인 `Backward Propagation`을 이해하는데 중요하기 때문에 시간이 될 때마다 연습을 하는 것이 좋습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coupled-productivity",
   "metadata": {
    "id": "brilliant-chapel"
   },
   "source": [
    "# 미분의 실사용 예시 :  경사하강법 (Gradient Descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggregate-drama",
   "metadata": {
    "id": "smooth-orbit"
   },
   "source": [
    "미분이 무엇인지, 파이썬으로는 어떻게 계산하는지, 어떤 때 필요한지 알아보았습니다. 마지막으로 최적화 알고리즘의 대표적인 예시인 경사하강법에 대해 알아보도록 하겠습니다. 경사하강법은 이후에 Section 2, 4에서도 다시 나올 예정이기 때문에 오늘 우리는 경사하강법이 무엇인지와 어떤 원리로 작동하는지에 대해서만 알아보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increased-handling",
   "metadata": {
    "id": "consecutive-carry"
   },
   "source": [
    "## 경사하강법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "similar-address",
   "metadata": {
    "id": "intense-diamond"
   },
   "source": [
    "경사하강법 (Gradient Descent, 이하 GD)는 위에서 거론 됐던 오차 함수인  𝜀 을 최소화 하는  𝑎,𝑏 를 찾을 수 있는 최적화 알고리즘 중의 하나입니다. 강의에서 최적의 a,b를 찾기 위해선 미분계수가 0인 곳을 찾으면 된다고 했습니다. 하지만 현실적으로 우리가 앞으로 다루게 될 문제에선 파라미터의 갯수는 수없이 많을 것이고 하나의 minimum/maximum만이 존재하지 않는 상황에 직면하게 될 것입니다. \n",
    "\n",
    "경사하강법은 임의의 a, b를 선택한 후 (random initialization)에 기울기 (gradient)를 계산해서 기울기 값이 낮아지는 방향으로 진행합니다. 기울기는 항상 손실 함수 값이 가장 크게 증가하는 방향으로 진행합니다. 그렇기 때문에 경사하강법 알고리즘은 기울기의 반대 방향으로 이동합니다.\n",
    "\n",
    "경사하강법에서 a,b는 다음과 같이 계산 됩니다 :\n",
    "\n",
    "$a_{n+1} = a_n - \\eta ∇ f(a_n)$ <br>\n",
    "$b_{n+1} = b_n - \\eta ∇ f(b_n)$\n",
    "\n",
    " 반복적으로 파라미터 a,b를 업데이트 해가면서 그래디언트($∇ f$)가 0이 될 때까지 이동을 합니다. 이 때 중요한게 바로 학습률 (learning rate, $\\eta$)인데요. 학습률이 너무 낮게 되면 알고리즘이 수렴하기 위해서 반복을 많이 해야되고 이는 결국 수렴에 시간을 상당히 걸리게 합니다. 반대로 학습률이 너무 크면 오히려 극소값을 지나쳐 버려서 알고리즘이 수렴을 못하고 계산을 계속 반복하게 될 수도 있기 때문에 학습률을 정할 때는 신중하게 정하셔야 합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "owned-edmonton",
   "metadata": {
    "id": "thick-adolescent"
   },
   "source": [
    "### 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "narrow-modification",
   "metadata": {
    "id": "charming-philip"
   },
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, lr = 0.05, epoch = 10):\n",
    "    \n",
    "    a, b = 0.33, 0.48 # 임의 선택한 파라미터 a, b\n",
    "    N = len(X) # 샘플 갯수\n",
    "    \n",
    "    for _ in range(epoch):            \n",
    "        f = y - (a*X + b)\n",
    "    \n",
    "        # a와 b를 업데이트 합니다\n",
    "        a -= lr * (-2 * X.dot(f).sum() / N)\n",
    "        b -= lr * (-2 * f.sum() / N)        \n",
    "        \n",
    "    print('a: '+str(a))\n",
    "    print('b: '+str(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crazy-cylinder",
   "metadata": {
    "id": "color-cycling"
   },
   "outputs": [],
   "source": [
    "# y = 3 x + 5\n",
    "X = np.array([1, 2, 3, 4, 5])\n",
    "y = np.array([8, 11, 14, 17, 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chicken-cleanup",
   "metadata": {
    "id": "compatible-wilson",
    "outputId": "cb3a741a-4411-4260-a065-bdbea524f2b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: 3.820070615328\n",
      "b: 2.0392842721280005\n"
     ]
    }
   ],
   "source": [
    "gradient_descent(X,y)  # 10번만 돌렸을 때"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affected-quarter",
   "metadata": {
    "id": "initial-champion",
    "outputId": "98e52168-2b3f-416b-e05a-0e0e58ddfa4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: 3.176787975497293\n",
      "b: 4.36173931393391\n"
     ]
    }
   ],
   "source": [
    "gradient_descent(X, y, epoch = 100)  # 100번 반복"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "built-arizona",
   "metadata": {
    "id": "greek-chapel",
    "outputId": "bdce9e5a-b455-4239-b4cb-549f88d7b2a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: 3.000000038323629\n",
      "b: 4.999999861639539\n"
     ]
    }
   ],
   "source": [
    "gradient_descent(X, y, epoch = 1000)  # 1000번 반복"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consistent-voluntary",
   "metadata": {
    "id": "accepting-yukon"
   },
   "source": [
    "점점 a = 3, b = 5로 수렴하시는 것을 보실 수 있습니다."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "n114-basic-derivative.ipynb의 사본",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
